{
  "Logic.Modal": {
    "description": "Demonstrates the fundamental duality between possibility (◇) and necessity (□) using Kripke semantics. It proves that 'P is possible' is logically equivalent to 'it is not the case that not-P is necessary'.",
    "imports": ["Philib.Logic.Modal"],
    "code": "open Philib.Logic\n\n-- Let's define a simple Kripke Frame with three worlds.\ninductive SimpleWorld | w1 | w2 | w3\n\n-- Define an accessibility relation R.\ndef simple_relation : SimpleWorld → SimpleWorld → Prop\n| w1, w2 => True  -- w1 can 'see' w2\n| w1, w3 => True  -- w1 can 'see' w3\n| w2, w3 => True  -- w2 can 'see' w3\n| _, _ => False\n\ndef simple_frame : KripkeFrame SimpleWorld where\n  R := simple_relation\n\n-- Define a proposition `p` that is true only at world w3.\ndef p : KripkeProp SimpleWorld\n| w3 => True\n| _  => False\n\n-- Theorem: At world w1, it is possible that p (◇p).\n-- This is because w1 can access w3, where p is true.\ntheorem p_is_possible_at_w1 : (diamond simple_frame p) SimpleWorld.w1 :=\nbegin\n  unfold diamond,\n  use SimpleWorld.w3,\n  apply And.intro,\n  { exact simple_relation SimpleWorld.w1 SimpleWorld.w3 },\n  { exact p SimpleWorld.w3 },\nend"
  },
  "Logic.Epistemic": {
    "description": "Illustrates the Truth Axiom (Axiom T) of epistemic logic. It proves that if knowledge is modeled on a reflexive accessibility relation (an agent always considers their actual world possible), then anything an agent knows must be true.",
    "imports": ["Philib.Logic.Epistemic"],
    "code": "open Philib.Logic.Epistemic\n\n-- Define two agents\ninductive Agent | Alice | Bob\n\n-- Define a set of worlds\ninductive World | sunny | rainy\n\n-- Define a reflexive accessibility relation for Alice\ndef alice_relation : World → World → Prop\n| sunny, sunny => True\n| rainy, rainy => True\n| _, _ => False\n\n-- Create an epistemic frame where only Alice's relation is defined this way\ndef my_frame : EpistemicFrame Agent World where\n  R := fun a w w' => if a = Alice then alice_relation w w' else False\n\n-- Axiom T requires the relation to be reflexive for the agent in question\nlemma alice_is_reflexive : ∀ (w : World), my_frame.R Alice w w := by intro w; cases w <;> simp [my_frame, alice_relation]\n\n-- Define a proposition `is_sunny`\ndef is_sunny : KripkeProp World\n| sunny => True\n| _     => False\n\n-- Theorem: We can prove the instance of Axiom T for Alice.\n-- If Alice knows it's sunny, it must actually be sunny.\ntheorem if_alice_knows_it_is_sunny_then_it_is_sunny :\n  IsValid ((Alice Knows is_sunny) → is_sunny) (frame := my_frame) :=\nbegin\n  -- We can directly apply our general proof of axiom_T\n  apply axiom_T (fun a w => if h : a = Alice then by rw [h]; apply alice_is_reflexive else by simp [my_frame]),\nend"
  },
  "Ontology.Core": {
    "description": "Demonstrates Leibniz's Law (The Indiscernibility of Identicals). It proves that if two individuals 'a' and 'b' are identical (a = b), then they must share all the same properties.",
    "imports": ["Philib.Ontology.Core"],
    "code": "open Philib.Ontology\n\n-- Let's consider individuals to be characters from a story\ninductive Character | Socrates | Plato\n\n-- Define a property: being a philosopher\ndef IsPhilosopher : Property Character\n| Socrates => True\n| Plato    => True\n\n-- Define another property: writing 'The Republic'\ndef WroteTheRepublic : Property Character\n| Plato => True\n| _     => False\n\n-- Assume we have two variables for individuals, and they are identical\nvariable (a b : Character)\naxiom h_identity : a = b\n\n-- Theorem: Given a = b, if a is a philosopher, then b must also be one.\ntheorem identity_implies_shared_philosophy :\n  instantiates a IsPhilosopher → instantiates b IsPhilosopher := \nbegin\n  -- We can use our general theorem from the library\n  intro h_a_is_philosopher,\n  rw [← h_identity],\n  exact h_a_is_philosopher,\nend"
  },
  "Ethics.Consequentialism": {
    "description": "Provides a concrete calculation using the utilitarian framework. It defines a simple choice scenario and proves that the action leading to the highest utility (saving more people) is the permissible one.",
    "imports": ["Philib.Ethics.Consequentialism"],
    "code": "open Philib.Ethics.Core Philib.Ethics.Consequentialism\n\n-- A simple scenario with two actions\ninductive SimpleAction | ActionA | ActionB\n\n-- States are just integer values representing outcomes\ndef SimpleState := Int\n\n-- Define consequence and utility functions\ndef my_consequence : ConsequenceFunction SimpleAction SimpleState\n| _, SimpleAction.ActionA => 10 -- Action A yields 10 utility\n| _, SimpleAction.ActionB => 5  -- Action B yields 5 utility\n\ndef my_utility : UtilityFunction SimpleState := fun s => s\n\n-- The agent must choose between A and B\ndef my_scenario : Scenario Unit SimpleAction where\n  agent := ()\n  initial_state := 0\n  available_actions := {SimpleAction.ActionA, SimpleAction.ActionB}\n\n-- Theorem: Action A is permissible because its utility (10) is >= any other choice.\ntheorem action_A_is_permissible :\n  is_permissible my_utility my_consequence my_scenario SimpleAction.ActionA := \nbegin\n  unfold is_permissible utility_of_action,\n  simp [my_scenario, my_consequence, my_utility],\n  -- The goal simplifies to checking that 5 ≤ 10 and 10 ≤ 10, which is true.\n  intro a' ha',\n  cases a'; simp,\nend"
  },
  "Ethics.Deontology": {
    "description": "Shows how the deontological framework operates. By assuming axioms about the (non-)universalizability of certain maxims (like 'lying to get a loan'), it proves that an action based on a non-universalizable maxim is impermissible.",
    "imports": ["Philib.Ethics.Deontology"],
    "code": "open Philib.Ethics.Core Philib.Ethics.Deontology\n\n-- Actions: to lie or to tell the truth\ninductive HonestyAction | Lie | TellTruth\n\n-- A simple state for a loan application scenario\nstructure LoanState where has_money : Bool\n\n-- Maxim of lying: Lie whenever it benefits you (e.g., to get a loan)\ndef maxim_of_lying : Maxim Unit HonestyAction LoanState := fun _ _ => HonestyAction.Lie\n\n-- Axiom: We assert that a world where everyone lies to get loans is inconceivable,\n-- because the institution of trust and lending would collapse.\naxiom lying_maxim_is_inconceivable : ¬ is_conceivable maxim_of_lying\n\n-- Theorem: The action of lying (in this context) is not permissible.\ntheorem lying_is_not_permissible :\n  ¬ (is_permissible maxim_of_lying HonestyAction.Lie () {has_money := false}) := \nbegin\n  unfold is_permissible,\n  -- The definition is `maxim ... = action ∧ is_conceivable ∧ is_rationally_willed`\n  -- We can prove its negation by showing `¬ is_conceivable`\n  intro h,\n  cases h with _ h_universalizable,\n  cases h_universalizable with h_conceivable _,\n  exact lying_maxim_is_inconceivable h_conceivable,\nend"
  },
  "Ethics.MetaTheories": {
    "description": "Highlights the core difference between Moral Realism and Emotivism. It proves that under Realism, a moral judgment is a proposition that must be either true or false. In contrast, under Emotivism, two opposing moral judgments do not form a logical contradiction.",
    "imports": ["Philib.Ethics.MetaTheories", "Philib.Ethics.Meta"],
    "code": "open Philib.Ethics.Meta Philib.Ethics.MetaTheories\n\n-- Let's consider a moral judgment about 'justice'\nstructure Justice (id : Nat) -- A subject for our judgment\n\ndef judgment_is_good : MoralJudgment where\n  subject := Justice 1\n  predicate := MoralPredicate.IsGood\n\ndef judgment_is_bad : MoralJudgment where\n  subject := Justice 1\n  predicate := MoralPredicate.IsBad\n\n/- Realist Perspective -/\n-- Let's define a simple realist theory where justice is objectively good\ndef my_realist_theory : MoralRealism where\n  moral_facts := fun _ => MoralPredicate.IsGood\n\n-- Theorem: Under this theory, the judgment 'justice is good' is a provable truth.\ntheorem justice_is_good_is_true_in_realism : eval_realist my_realist_theory judgment_is_good := by simp [eval_realist, my_realist_theory, judgment_is_good]\n\n/- Emotivist Perspective -/\n-- Theorem: Under emotivism, stating 'justice is good' and 'justice is bad' are\n-- expressions of conflicting attitudes, not a logical contradiction (like `p ∧ ¬p`).\n-- The proof simply shows that Approval ≠ Disapproval.\ntheorem emotivist_conflict_is_not_contradiction : \n  eval_emotivist judgment_is_good ≠ eval_emotivist judgment_is_bad := \nbegin\n  unfold eval_emotivist,\n  simp [judgment_is_good, judgment_is_bad],\nend"
  }
}
